{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcGTtrF+95Igun+4sLLj1J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaunak-badani/XAI/blob/main/RL/Reinforcement_Learning_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kThY9qUakrRN"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install -q gym-super-mario-bros==7.4.0 tensordict==0.7.2 torchrl==0.7.0\n",
        "pip install -q 'numpy<2.0.0'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import random, datetime, os\n",
        "\n",
        "# Gym is an OpenAI toolkit for RL\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "\n",
        "# NES Emulator for OpenAI Gym\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "from gym import spaces\n",
        "\n",
        "from tensordict import TensorDict\n",
        "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage"
      ],
      "metadata": {
        "id": "gla-SdSgkyg-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(SimpleEnv, self).__init__()\n",
        "\n",
        "        # Action space: Space with 5 actions\n",
        "        self.action_space = spaces.Discrete(5)\n",
        "        print(self.action_space)\n",
        "\n",
        "        # Observation space: 2D box of dimensions 8x8\n",
        "        self.observation_space = spaces.Box(low=-4, high=4, shape = (8, 8), dtype=np.uint8)\n",
        "        print(self.observation_space)\n",
        "\n",
        "        # Initial state\n",
        "        self.state = self.get_initial_random_state()\n",
        "        self.nsteps = 0\n",
        "\n",
        "    def get_initial_random_state(self, number_of_dirt_pieces = 15):\n",
        "        board = np.zeros(shape = (8, 8))\n",
        "        x_indices = np.random.randint(0, 8, size = number_of_dirt_pieces)\n",
        "        y_indices = np.random.randint(0, 8, size = number_of_dirt_pieces)\n",
        "        board[x_indices, y_indices] = 1\n",
        "        starting_position = np.random.randint(0, 8, size = 2)\n",
        "        board[starting_position[0], starting_position[1]] = 0\n",
        "        return {'board': board, 'position': starting_position }\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the environment to an initial state.\"\"\"\n",
        "        self.state = self.get_initial_random_state()\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Apply the action and return the next state, reward, done, and info.\"\"\"\n",
        "        # Simple logic for the environment:\n",
        "        dx = [1, -1, 0, 0]\n",
        "        dy = [0, 0, 1, -1]\n",
        "        print(state)\n",
        "        state = self.state\n",
        "        if action < 4:\n",
        "          state['position'][0] += dx[action]\n",
        "          state['position'][1] += dy[action]\n",
        "\n",
        "\n",
        "        # Reward function\n",
        "        # reward = -abs(self.state)  # Negative reward for being far from the center (0)\n",
        "\n",
        "        reward = 1\n",
        "        self.nsteps += 1\n",
        "        # Check if the episode is done\n",
        "        done = self.nsteps >= 200  # Terminate if the state goes out of bounds\n",
        "\n",
        "        return state, reward, done, {}\n",
        "\n",
        "    def render(self, mode=\"human\"):\n",
        "        \"\"\"Render the environment state.\"\"\"\n",
        "        print(f\"Current state: {self.state}\")\n",
        "\n",
        "env = SimpleEnv()\n",
        "env.step(2)\n",
        "env.render()"
      ],
      "metadata": {
        "id": "2ZWHhhHFlZHm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2f63bf5-1bff-4d17-cbd5-e351a51f0d7e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(5)\n",
            "Box(252, 4, (8, 8), uint8)\n",
            "Current state: {'board': array([[0., 0., 0., 1., 0., 0., 0., 1.],\n",
            "       [0., 0., 1., 0., 0., 1., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 1., 0., 0., 0., 0., 1., 1.],\n",
            "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "       [1., 0., 0., 1., 1., 0., 0., 1.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0.]]), 'position': array([1, 8])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Cr6UQLGmG91"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}