{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6a7j2u-cjugI"
   },
   "source": [
    "# AIPI 590 - XAI | Assignment #05\n",
    "## Explainable Deep Learning\n",
    "## Shaunak Badani\n",
    "\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/shaunak-badani/XAI/blob/main/Assignment05/ExplainableDeepLearning.ipynb)\n",
    "\n",
    "> This notebook tests a hypothesis on the IMDB dataset using Integrated gradients, and then shows the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7U3HdfTvkJLi",
    "outputId": "e265fb05-4571-45bb-ea15-d8c36ffdd3fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: alibi[tensorflow] in /usr/local/lib/python3.11/dist-packages (0.9.6)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from alibi[tensorflow]) (1.26.4)\n",
      "Requirement already satisfied: pandas<3.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from alibi[tensorflow]) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from alibi[tensorflow]) (1.6.1)\n",
      "Requirement already satisfied: spacy<4.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (3.7.5)\n",
      "Requirement already satisfied: blis<0.8.0 in /usr/local/lib/python3.11/dist-packages (from alibi[tensorflow]) (0.7.11)\n",
      "Requirement already satisfied: scikit-image<0.23,>=0.17.2 in /usr/local/lib/python3.11/dist-packages (from alibi[tensorflow]) (0.22.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from alibi[tensorflow]) (2.32.3)\n",
      "Requirement already satisfied: Pillow<11.0,>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from alibi[tensorflow]) (10.4.0)\n",
      "Requirement already satisfied: attrs<24.0.0,>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from alibi[tensorflow]) (23.2.0)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from alibi[tensorflow]) (1.13.1)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from alibi[tensorflow]) (3.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from alibi[tensorflow]) (4.12.2)\n",
      "Requirement already satisfied: dill<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from alibi[tensorflow]) (0.3.9)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from alibi[tensorflow]) (4.48.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.28.1 in /usr/local/lib/python3.11/dist-packages (from alibi[tensorflow]) (4.67.1)\n",
      "Requirement already satisfied: tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from alibi[tensorflow]) (2.14.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi[tensorflow]) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi[tensorflow]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi[tensorflow]) (4.55.8)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi[tensorflow]) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi[tensorflow]) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi[tensorflow]) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi[tensorflow]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=1.0.0->alibi[tensorflow]) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=1.0.0->alibi[tensorflow]) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.21.0->alibi[tensorflow]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.21.0->alibi[tensorflow]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.21.0->alibi[tensorflow]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.21.0->alibi[tensorflow]) (2025.1.31)\n",
      "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.11/dist-packages (from scikit-image<0.23,>=0.17.2->alibi[tensorflow]) (3.4.2)\n",
      "Requirement already satisfied: imageio>=2.27 in /usr/local/lib/python3.11/dist-packages (from scikit-image<0.23,>=0.17.2->alibi[tensorflow]) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image<0.23,>=0.17.2->alibi[tensorflow]) (2025.1.10)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /usr/local/lib/python3.11/dist-packages (from scikit-image<0.23,>=0.17.2->alibi[tensorflow]) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0.0,>=1.0.0->alibi[tensorflow]) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0.0,>=1.0.0->alibi[tensorflow]) (3.5.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (0.15.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (3.1.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (75.1.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (3.5.0)\n",
      "Requirement already satisfied: spacy-lookups-data<1.1.0,>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (1.0.5)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (25.1.24)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (4.25.6)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (2.5.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (0.37.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (2.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (2.14.0)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (2.14.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.7.0->alibi[tensorflow]) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.7.0->alibi[tensorflow]) (0.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.7.0->alibi[tensorflow]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.7.0->alibi[tensorflow]) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.7.0->alibi[tensorflow]) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.7.0->alibi[tensorflow]) (0.5.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (0.45.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers<5.0.0,>=4.7.0->alibi[tensorflow]) (2024.10.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (2.27.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (3.1.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (3.0.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (2.0.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=2.0.0->spacy[lookups]<4.0.0,>=2.0.0->alibi[tensorflow]) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow!=2.6.0,!=2.6.1,<2.15.0,>=2.0.0->alibi[tensorflow]) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install alibi[tensorflow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-NJO9JP-huK8",
    "outputId": "a56bad89-63aa-4fde-d316-b51fa6e5714c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version:  2.14.1\n",
      "Eager execution enabled:  True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Conv1D, GlobalMaxPooling1D, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from alibi.explainers import IntegratedGradients\n",
    "import matplotlib.pyplot as plt\n",
    "print('TF version: ', tf.__version__)\n",
    "print('Eager execution enabled: ', tf.executing_eagerly()) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Lna1dsVRjX3f"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wq_K-AWmlI4a",
    "outputId": "96c18fd4-71bb-427a-8333-517eec37e12b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sequences: 25000\n",
      "Number of testing sequences: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training sequences: {len(x_train)}\")\n",
    "print(f\"Number of testing sequences: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EA5HWQd3CRVK"
   },
   "outputs": [],
   "source": [
    "# Padding sequences such that all sequences have the same length\n",
    "max_length = 100\n",
    "x_train = sequence.pad_sequences(x_train, maxlen = max_length)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VJFa59lgHP78"
   },
   "outputs": [],
   "source": [
    "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3bh9uYjTlOgd"
   },
   "outputs": [],
   "source": [
    "index = imdb.get_word_index()\n",
    "reverse_index = {value: key for (key, value) in index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bSKC4GMS5y3m"
   },
   "outputs": [],
   "source": [
    "def decode_sentence(x, reverse_index):\n",
    "    return \" \".join([reverse_index.get(i - 3, 'UNK') for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "JpuNBs4GG63e",
    "outputId": "6410042a-cb91-493e-9ab7-acb14318eb3e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"you into your hearts but really it ended and i felt like i had watched a 5 minute cartoon on kids tv br br i don't have children of my own but when i do i fully intend to show them quality children's movies like the movie toy story and finding UNK even though they are too childish for me these days i can see how they would be of great appeal to young children not so with this appalling attempt at a movie br br oh and one more thing not enough UNK he should have his own movie\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(x_test[50], reverse_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcIFyzTG2uoN"
   },
   "source": [
    "## Building the model\n",
    "\n",
    "- We'll be training a 1 dimensional convolutional nueral network, with pooling.\n",
    "\n",
    "- This model will be used with the Integrated Gradients method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "N_xutlnklnAg"
   },
   "outputs": [],
   "source": [
    "sequence_len = max_length\n",
    "features = 10000\n",
    "d_embed = 50\n",
    "inputs = Input(shape = (sequence_len, ), dtype = tf.int32)\n",
    "embedded_sequences = Embedding(features, d_embed)(inputs)\n",
    "\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "out = Conv1D(filters, kernel_size, padding = 'valid', activation = 'relu', strides = 1)(embedded_sequences)\n",
    "out = Dropout(0.4)(out)\n",
    "out = GlobalMaxPooling1D()(out)\n",
    "out = Dense(hidden_dims, activation = 'relu')(out)\n",
    "out = Dropout(0.4)(out)\n",
    "\n",
    "outputs = Dense(2, activation = 'softmax')(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6AdftqFSImIy",
    "outputId": "5ced8f04-e5b8-4fcb-fba7-a9413e410039"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oHiBoqIpHVRS",
    "outputId": "49d09feb-d2be-44a4-83d7-6ff086e0758d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZW3bqFSm4WQP",
    "outputId": "c0e85379-6c3d-49cf-9604-bd0ec7aecb3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "98/98 [==============================] - 39s 378ms/step - loss: 0.5956 - accuracy: 0.6671 - val_loss: 0.4191 - val_accuracy: 0.8181\n",
      "Epoch 2/3\n",
      "98/98 [==============================] - 32s 323ms/step - loss: 0.3180 - accuracy: 0.8644 - val_loss: 0.3441 - val_accuracy: 0.8541\n",
      "Epoch 3/3\n",
      "98/98 [==============================] - 35s 356ms/step - loss: 0.2131 - accuracy: 0.9183 - val_loss: 0.3394 - val_accuracy: 0.8518\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7c907873ac10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(inputs=inputs, outputs = outputs)\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size = 256, epochs = 3, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teFxCAdlNNsa"
   },
   "source": [
    "# Integrated gradients\n",
    "\n",
    "- Each word in the sequence is mapped to a 50 dimensional vector.\n",
    "- For a 100-length sequence, this will amount to a 100 x 50 dimension matrix.\n",
    "- So  the attribution matrix will also be a matrix of length 100 x 50\n",
    "- If N samples are used, then attribution tensor = (N, 100, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19wu_sgt5rnZ",
    "outputId": "9983c7d7-ad2a-4583-9502-99182ba95a54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.embedding.Embedding at 0x7c90798ff350>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = model.layers[1]\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "TtZ2SCheP5Ab"
   },
   "outputs": [],
   "source": [
    "n_steps = 50\n",
    "method = \"gausslegendre\"\n",
    "internal_batch_size = 100\n",
    "nb_samples = 10\n",
    "ig  = IntegratedGradients(model,\n",
    "                          layer=layer,\n",
    "                          n_steps=n_steps,\n",
    "                          method=method,\n",
    "                          internal_batch_size=internal_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_CWT3gwgP6jF"
   },
   "outputs": [],
   "source": [
    "test_batch = x_test[:nb_samples]\n",
    "probabilities = model(test_batch).numpy()\n",
    "preds = probabilities.argmax(axis = 1)\n",
    "explanation = ig.explain(test_batch, baselines = None, target=preds, attribute_to_layer_inputs = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T2qKhUuTP72c",
    "outputId": "4e96b3f9-ffa5-45ff-a075-b801c8f0f740"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'IntegratedGradients',\n",
       " 'type': ['whitebox'],\n",
       " 'explanations': ['local'],\n",
       " 'params': {'target_fn': None,\n",
       "  'method': 'gausslegendre',\n",
       "  'n_steps': 50,\n",
       "  'internal_batch_size': 100,\n",
       "  'layer': 1},\n",
       " 'version': '0.9.6'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanation.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DT_lc317Qaas",
    "outputId": "6ea410a5-f994-43d1-cc02-5ed791cb8dbc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 100, 50)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanation.attributions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "WSHBljDqQy7v"
   },
   "outputs": [],
   "source": [
    "attrs = explanation.attributions[0]\n",
    "attrs = attrs.sum(axis = 2) # Sum along all dimensions of the vector embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nv7lUYJNRZvT"
   },
   "source": [
    "### Visualize the attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "GWlv2yAeRFeQ"
   },
   "outputs": [],
   "source": [
    "sample_sentence_no = 4\n",
    "x_i = test_batch[sample_sentence_no]\n",
    "attrs_i = attrs[sample_sentence_no]\n",
    "pred = preds[sample_sentence_no]\n",
    "pred_dict = {1: 'Positive review', 0: 'Negative review'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJk1XTWiSh62",
    "outputId": "fca018b1-4514-451b-e909-40ebf31245fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label =  1: Positive review\n"
     ]
    }
   ],
   "source": [
    "print('Predicted label =  {}: {}'.format(pred, pred_dict[pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "jmvt16qwSpKE"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "def  hlstr(string, color='white'):\n",
    "    \"\"\"\n",
    "    Return HTML markup highlighting text with the desired color.\n",
    "    \"\"\"\n",
    "    return f\"<mark style=background-color:{color}>{string} </mark>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "xQpWG5chStcX"
   },
   "outputs": [],
   "source": [
    "def colorize(attrs, cmap='PiYG'):\n",
    "    \"\"\"\n",
    "    Compute hex colors based on the attributions for a single instance.\n",
    "    Uses a diverging colorscale by default and normalizes and scales\n",
    "    the colormap so that colors are consistent with the attributions.\n",
    "    \"\"\"\n",
    "    import matplotlib as mpl\n",
    "    cmap_bound = np.abs(attrs).max()\n",
    "    norm = mpl.colors.Normalize(vmin=-cmap_bound, vmax=cmap_bound)\n",
    "    cmap = mpl.cm.get_cmap(cmap)\n",
    "\n",
    "    # now compute hex values of colors\n",
    "    colors = list(map(lambda x: mpl.colors.rgb2hex(cmap(norm(x))), attrs))\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PheNtgQKSuth",
    "outputId": "8c9110af-4df6-48d5-f128-140e409c5aef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-a4396b7e2a75>:10: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap = mpl.cm.get_cmap(cmap)\n"
     ]
    }
   ],
   "source": [
    "words = decode_sentence(x_i, reverse_index).split()\n",
    "colors = colorize(attrs_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "7Bj2YjGPSvmc",
    "outputId": "0daeca0f-69d5-492c-cfca-60ff370577f2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<mark style=background-color:#f7f7f6>die </mark><mark style=background-color:#f7f7f6>hard </mark><mark style=background-color:#f7f7f7>mario </mark><mark style=background-color:#f7f7f6>fan </mark><mark style=background-color:#eef6e2>and </mark><mark style=background-color:#f3f7ef>i </mark><mark style=background-color:#98cc5f>loved </mark><mark style=background-color:#f0f6e7>this </mark><mark style=background-color:#f1f6ea>game </mark><mark style=background-color:#f7f7f6>br </mark><mark style=background-color:#f7f7f6>br </mark><mark style=background-color:#f8f2f5>this </mark><mark style=background-color:#f6f7f5>game </mark><mark style=background-color:#f9d1e8>starts </mark><mark style=background-color:#f1f6ea>slightly </mark><mark style=background-color:#d4539b>boring </mark><mark style=background-color:#f6f7f5>but </mark><mark style=background-color:#f5f7f3>trust </mark><mark style=background-color:#f6f7f5>me </mark><mark style=background-color:#f4f7f0>it's </mark><mark style=background-color:#f6f7f5>worth </mark><mark style=background-color:#f5f7f2>it </mark><mark style=background-color:#f6f7f5>as </mark><mark style=background-color:#f5f7f3>soon </mark><mark style=background-color:#f3f7ef>as </mark><mark style=background-color:#f3f7ef>you </mark><mark style=background-color:#f8f4f6>start </mark><mark style=background-color:#f7f6f7>your </mark><mark style=background-color:#f0f6e7>hooked </mark><mark style=background-color:#f7f7f7>the </mark><mark style=background-color:#f2f6ec>levels </mark><mark style=background-color:#f5f7f2>are </mark><mark style=background-color:#eef6e2>fun </mark><mark style=background-color:#f1f6ea>and </mark><mark style=background-color:#f7f7f7>UNK </mark><mark style=background-color:#f7f6f7>they </mark><mark style=background-color:#f5f7f3>will </mark><mark style=background-color:#f7f7f7>hook </mark><mark style=background-color:#f7f7f7>you </mark><mark style=background-color:#f7f7f7>UNK </mark><mark style=background-color:#f9f0f5>your </mark><mark style=background-color:#f8f4f6>mind </mark><mark style=background-color:#f7f7f6>turns </mark><mark style=background-color:#f7f7f6>to </mark><mark style=background-color:#f8f4f6>UNK </mark><mark style=background-color:#f9eef4>i'm </mark><mark style=background-color:#fbe9f2>not </mark><mark style=background-color:#faebf3>kidding </mark><mark style=background-color:#f8f4f6>this </mark><mark style=background-color:#f0f6e7>game </mark><mark style=background-color:#f3f6ed>is </mark><mark style=background-color:#eff6e4>also </mark><mark style=background-color:#f6f7f5>UNK </mark><mark style=background-color:#f5f7f2>and </mark><mark style=background-color:#edf6df>is </mark><mark style=background-color:#b7e085>beautifully </mark><mark style=background-color:#f5f7f2>done </mark><mark style=background-color:#f7f7f7>br </mark><mark style=background-color:#f7f7f7>br </mark><mark style=background-color:#f7f7f6>to </mark><mark style=background-color:#f7f7f6>keep </mark><mark style=background-color:#f7f6f7>this </mark><mark style=background-color:#f7f7f7>spoiler </mark><mark style=background-color:#f9eff4>free </mark><mark style=background-color:#f8f5f6>i </mark><mark style=background-color:#f7f7f7>have </mark><mark style=background-color:#f7f7f6>to </mark><mark style=background-color:#f7f7f6>keep </mark><mark style=background-color:#f8f3f6>my </mark><mark style=background-color:#f9f1f5>mouth </mark><mark style=background-color:#f8f5f6>shut </mark><mark style=background-color:#f7f6f7>about </mark><mark style=background-color:#f7f6f7>details </mark><mark style=background-color:#f8f5f6>but </mark><mark style=background-color:#faedf3>please </mark><mark style=background-color:#fbe9f2>try </mark><mark style=background-color:#f9f0f5>this </mark><mark style=background-color:#f7f7f6>game </mark><mark style=background-color:#f7f6f7>it'll </mark><mark style=background-color:#f7f6f7>be </mark><mark style=background-color:#f9f0f5>worth </mark><mark style=background-color:#f7f7f6>it </mark><mark style=background-color:#f7f7f6>br </mark><mark style=background-color:#f5f7f2>br </mark><mark style=background-color:#f4f7f0>story </mark><mark style=background-color:#589b28>9 </mark><mark style=background-color:#276419>9 </mark><mark style=background-color:#d9f0bc>action </mark><mark style=background-color:#a9d874>10 </mark><mark style=background-color:#f7cbe4>1 </mark><mark style=background-color:#f7f7f6>it's </mark><mark style=background-color:#f7f7f7>that </mark><mark style=background-color:#f5f7f3>good </mark><mark style=background-color:#f7f6f7>UNK </mark><mark style=background-color:#faecf3>10 </mark><mark style=background-color:#f4f7f0>attention </mark><mark style=background-color:#f8f2f5>UNK </mark><mark style=background-color:#f1b7da>10 </mark><mark style=background-color:#f1f6ea>average </mark><mark style=background-color:#d6eeb6>10 </mark>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\".join(list(map(hlstr, words, colors))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLMrHRIhTv6S"
   },
   "source": [
    "### My intuition says that you don't need the whole text to generate reviews. The first 20 words are enough.\n",
    "\n",
    "- H0: The model doesn't show significant difference if 100 words are passed or 20 words are passed.\n",
    "\n",
    "- H1: The model does show significant difference if 100 words are passed instead of the first 20.\n",
    "\n",
    "In order to test this, I will be using the paired t-test.\n",
    "This is because attribution scores for the same review are likely going to be dependent, and hence assuming normality.\n",
    "\n",
    "I will take the average of all of the attributions of the first 20 words, and then an average attribution of the remaining 80 words. And using multiple of these reviews, a paired t-test would be conducted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "6RvDFQVBSycJ"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ztsbRtc9W5RI",
    "outputId": "30a27196-ac69-4a7c-b427-f8e4febf7a4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 100)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "attrs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "fMsbnxKcTUg5"
   },
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "\n",
    "ig  = IntegratedGradients(model,\n",
    "                          layer=layer,\n",
    "                          n_steps=n_steps,\n",
    "                          method=method,\n",
    "                          internal_batch_size=internal_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "7mmW8FmAXG8r"
   },
   "outputs": [],
   "source": [
    "test_batch = x_test[:num_samples]\n",
    "probabilities = model(test_batch).numpy()\n",
    "preds = probabilities.argmax(axis = 1)\n",
    "explanation = ig.explain(test_batch, baselines = None, target=preds, attribute_to_layer_inputs = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "T5KAiEoPXPJ5"
   },
   "outputs": [],
   "source": [
    "attrs = explanation.attributions[0]\n",
    "word_attrs = attrs.sum(axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "8hYVQG0VZTV1"
   },
   "outputs": [],
   "source": [
    "first_20_words_mean_attr = word_attrs[:, :20].mean(axis = -1)\n",
    "rem_80_words_mean_attr = word_attrs[:, 20:].mean(axis = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "xM8JPogzZaQa"
   },
   "outputs": [],
   "source": [
    "_, p_value = ttest_rel(first_20_words_mean_attr, rem_80_words_mean_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SdVlZ_VxZvbW",
    "outputId": "117b8a44-7933-4a64-b7f3-f888737b2418"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.063349008418896"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEp83NlwbHCM"
   },
   "source": [
    "Since p-value >= 0.05, we fail to reject the null hypothesis, meaning that the first 20 words may be enough to predict the review of the movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHUDdwgzbScp"
   },
   "source": [
    "# AI Usage\n",
    "\n",
    "- AI was used to get ideas of how to conduct the tests, for e.g. Chatgpt was used at 6:10 pm on Feb 12 to generate ideas of the mathematical ways of using the attributions and input them to a paired t-test.\n",
    "- AI was NOT used to generate any of the code in this notebook."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNvmRNcB0EeGGR1LOs/8DO1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
